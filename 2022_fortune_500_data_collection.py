# -*- coding: utf-8 -*-
"""2022-fortune-500-data-collection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10Mu265K4d6ZdgUXiykaChkTz6JE9JXoS

# Fortune 500 Data Collection

## Overview

The notebook is broken down as follows:
1.  **Network Analysis**
    * Explore the network and data requests used to render our webpage
    * Call the API that supplies the data for the company table   
2.  **JSON Data**
    * Parse the JSON response from the API and select the data we want
    * Create a dataframe and organize data  
3.  **Scraping URLs**
    * Crawl all the individual company Fortune webpages
    * Find the html that contains the data we want and extract it
    * Scrape our data and create a dataframe   
4.  **Joining and Sorting Data**
    * Join our two datasets and sort the rows  
5.  **Conclusion**
    * Convert dataframe to csv file

# 1. Network Analysis

I want to create a dataset of all the companies in the Fortune 500 with some specific information about each. I figure the best way to do that is to go to the actual Fortune Magazine website. 

I planned on scraping information from the list of companies, but unfortunately this is a webpage heavy with javascript where the list changes without reloading the page. This prevents me from just scraping all the list contents direcly from the html.

Since traditional web scraping won't work, I suspect there is another source producing the information on this web page. It is likely an API that my browser is calling to get some sort of JSON response and render the information on the page. So I decided to try to find the API that responds with the company data listed in this table. 

To do this we need to right-click on the web page and select "inspect", which will pull up the developer window on our webpage. Then go to the network tab, which will look something like this:
"""

!pip install bs4
import os
import time
import sys
import numpy as np
import pandas as pd
import regex as re
import lxml
import numbers
from bs4 import BeautifulSoup
import requests
import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from collections import defaultdict

req_headers = {
    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
    'accept-encoding': 'gzip, deflate, br',
    'accept-language': 'en-US,en;q=0.8',
    'upgrade-insecure-requests': '1',
    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36'
}

"""We want to look at the XHR responses in the network tab and try to find any response from a url name that mentions "fortune" or "lists". We poke around a little and find a GET request to the url below that provides us with a JSON repsone. """

page_url = 'https://content.fortune.com/wp-json/irving/v1/data/franchise-search-results?list_id=3287962&token=Zm9ydHVuZTpCcHNyZmtNZCN5SndjWkkhNHFqMndEOTM='

"""We send a request to the url and it provides us with a JSON response that contains all sorts of data on each company in the list! """

# request the URL and parse the JSON
response = requests.get(page_url)
response.raise_for_status() # raise exception if invalid response
comps = response.json()
print(len(comps))

"""**Now we can begine exploring and parsing the data we want.** 

- Above we see what we actually got back from our request was a list that contained 2 items.

- The second item in the list contains the data we want so we call that one.
"""

comp = comps[1]
print(len(comp))

"""Let's see if we can get a better view of the json data for this company."""

# prettify json
format_json = json.dumps(comp, sort_keys=True, indent=5)
print(format_json[:1000])

"""Our comp (companies) variable contains a list of dictionaries for each company. Each company has two parts: 1) a list of dictionaries which contain information about the company, and 2) a dictionary with a link to the company's Fortune page.

### Let's give an example of one of the companies below
"""

# select the third company
n_comp = comp['items'][2]
print(n_comp)

"""The third company in the list is Xcel Energy and it's dictionary has two parts:

1. A "fields" dictionary
2. A "permalink" dictionary
"""

n_comp.keys()

"""The permalink dictionary returns a link to the company's Fortune webpage."""

new_comp = n_comp['permalink']
print(new_comp)

"""The "fields" dictionary returns a list of other dictionaries containing items which populate the main Fortune web page. """

my_comp = n_comp['fields']
print(my_comp)

"""# 2. JSON Data

Now that we know how our dataset looks, we're going to select info about each company it contains. 

First, grab all of the "field" dictionaries from comp dataset and put them in list.
"""

fields = []

for j in comp['items']:
    
    #for key, value in comp:
    field = j['fields']
    fields.append(field)
        
print(len(fields))

"""There are 1000 companies in this list.

### DataFrame

Now we're going to pull the data we want from each company's dictionary and put it into a dataframe.

Create a dataframe and select the features we want from the json output.
"""

df = pd.DataFrame()

def gather_data(name, col):

    lst = []
    val = ""
    for i in fields:
        for j in i:
            if j['key'] == name:
                val = j['value']
            else:
                pass
        
        lst.append(val)
        
    df[col] = lst
    
    return df

keys = ['name', 'rank', 'rankchange', 'f500_revenues', 'f500_profits', 
        'f500_employees', 'sector', 'hqcity', 'hqstate', 'newcomer', 
        'ceofounder', 'ceowoman', 'profitable']

cols = ['company', 'rank', 'rank_change', 'revenue', 'profit',
        'num. of employees', 'sector', 'city', 'state', 'newcomer', 
        'ceo_founder', 'ceo_woman', 'profitable']


for x,y in zip(keys, cols):
    f500_df = gather_data(x, y)

print(f500_df.shape)
f500_df.head()

"""### Data Types and Features

Change our data types to numeric form in columns that show numbers.
"""

f500_df[['rank', 'rank_change', 'revenue', 'profit','num. of employees']] = f500_df[['rank', 'rank_change', 'revenue','profit','num. of employees']].apply(pd.to_numeric)

"""Fill missing rank change values with zero because they either didn't change ranks or they are newcommers. """

# fill missing values with zero
f500_df['rank_change'] = f500_df['rank_change'].fillna(0)

"""Check data types"""

print(f500_df.dtypes)

"""**New Feature: Previous Rank** 

Now we're going to create a new feature that shows the previous rank (2020) of each company in the Forune 500. Does not apply to companies outside of top 500. 
"""

# Create new features
f500_df['prev_rank'] = (f500_df['rank'] + f500_df['rank_change'])
f500_df.head()

"""But we need to account for companies that are newcommers to the 500 and any companies outside of the top 500 (their rank change is not given). If we don't we will get duplicate values. """

# newcommer prev_rank
f500_df.loc[(f500_df['newcomer'] == 'yes')| (f500_df['rank']>499), 'prev_rank'] = " "

f500_df.head()

"""And that is our first dataset.

# 3. Crawl Company Urls

Now we can address the "permalink" varibale in our dictionaries. 

This dictionary contains the link to each company's Fortune webpage.

Grab those links and put them in a list.

Gather urls:
"""

urls = []

for i in comp['items']:
    
    #for key, value in comp:
    url = i['permalink']
    urls.append(url)
        
print(len(urls))

"""Crawl each of these urls and pull the infromation from the page for:

- website url
- CEO
- Ticker
- Market cap

For each company in the list, we will crawl their Fortune 500 company page and scrape the information we desire.

### Another Roadblock

Interestingly, the data we want cannot be scraped by simply looking at the developer window and calling the elements representing the corresponding values. You can see in the photo below, the CEO name is represented by a "div" element and in the "info__value--2AHH7" class, but when we call those elements it returns an empty dataset.

![ceo_dev_shot.png](attachment:ceo_dev_shot.png)

Looks like I need to be a little more creative, so I pulled up the page source and did a keyword search on the the html. I found the name of the CEO in the html and going backwareds I was able to determine it was defined inside of the javascript variable:

    "__PRELOADED_STATE__ = "

![html_search.png](attachment:html_search.png)

Therefore, we create our function to make our webpage request and then we extract our variable "__PRELOADED_STATE__". We can further narrow our response to the actual dictionary, **"company-information"**, that holds the data we seek. And finally, we convert that data to json format.
"""

def soups(data):
    r = requests.get(data).text
    # select the the JSON from the html
    html = r[r.find('__PRELOADED_STATE__ = ') + len('__PRELOADED_STATE__ = '):]
    html = html[:html.find('};') + 1]
    
    # find company-information
    good_data = html[html.find('"company-information","config":') + len('"company-information","config":'):]
    good_data = good_data[:good_data.find('},') + 1]
    
    # convert to json
    j = json.loads(good_data)
    
    return j

# Call soup function and store output in a list
web = []

for url in urls:
    htmls = soups(url)
    web.append(htmls)
print(len(web))

"""**Example:** Second company in the list web page response."""

print(web[1])

"""### Scrape Data and Store in Dataframe

Next, we scrape the specific datsa we want from each dictionary in our list and put the data into a new dataframe. 
"""

m_df = pd.DataFrame()

def get_data(name, col):

    lst = []
    val = ""
    for i in web: 
        for key, value in i.items():
            if key == name:
                val = value
            else:
                pass
        
        lst.append(val)
        
    m_df[col] = lst
    
    return m_df

keys = ['ceo','website', 'ticker','marketValue']

cols = ['CEO', 'Website', 'Ticker','Market Cap']


for x,y in zip(keys, cols):
    m_df = get_data(x, y)
    #n_df = get_data(x, y)

m_df.head()

"""And this makes up our second dataset.

# 4. Join and Sort

Now we can join our two datasets and sort the data so it shows the rankings from 1 to 1000.
"""

# join
f1000_df = f500_df.join(m_df, how='left')

print(f1000_df.shape)
f1000_df.head()

"""**Sort**"""

f1000_df = f1000_df.sort_values(by='rank', ignore_index=True)

f1000_df.head()

"""# Conclusion

Finally, we have our complete Fortune 500 dataset and we can create our csv file. 
"""

f1000_df.to_csv("Fortune_1000.csv", index=False)

"""#Data Analysis

Validating the data using matplotlib
"""

f1000_df.head()

print(f1000_df.dtypes)

f1000_df.rename(columns = {'num. of employees':'empCount'}, inplace = True)
f1000_df.rename(columns = {'Market Cap':'MarketCap'}, inplace = True)

f1000_df['empCount'] = f1000_df['empCount'].replace(np.nan, 0) 
f1000_df['profit'] = f1000_df['profit'].replace(np.nan, 0) 
f1000_df['rank_change'] = f1000_df['rank_change'].replace(np.nan, 0)
f1000_df['revenue'] = f1000_df['revenue'].replace(np.nan, 0)
f1000_df['prev_rank'] = f1000_df['prev_rank'].replace(np.nan, 0)
f1000_df['prev_rank'] = f1000_df['prev_rank'].replace(' ', 0)
f1000_df['MarketCap'] = f1000_df['MarketCap'].replace(np.nan, 0)
f1000_df['MarketCap'] = f1000_df['MarketCap'].replace(' ', 0)
f1000_df['MarketCap'] = f1000_df['MarketCap'].replace('-', 0)

f1000_df.rank_change = f1000_df.rank_change.astype(int)
f1000_df.profit = f1000_df.profit.astype(int)
f1000_df.revenue = f1000_df.revenue.astype(int)
f1000_df.empCount = f1000_df.empCount.astype(int)
f1000_df.prev_rank = f1000_df.prev_rank.astype(int)
# show the datatypes
print(f1000_df.dtypes)

"""# Visualization
 Presenting and Validating data using matplotlib
"""

# Mounting the drive
from google.colab import drive
drive.mount('/content/drive')

# Reading the scraped Data
df = pd.read_csv('Fortune_1000.csv')
df.shape
print(df)

df2 = df.groupby('sector')[['num. of employees']].mean().sort_values(['num. of employees'],ascending=False)
df2.reset_index(inplace=True)
plt.figure(figsize=(10,10))
chart = sns.barplot(data = df2, x='sector', y='num. of employees')
chart.set_xticklabels(chart.get_xticklabels(), rotation=90, horizontalalignment='right', fontweight='light')
chart.axes.yaxis.label.set_text("num. of employees")

df2 = df.groupby('sector')[['revenue']].mean().sort_values(['revenue'],ascending=False)
df2.reset_index(inplace=True)
plt.figure(figsize=(10,10))
chart = sns.barplot(data = df2, x='sector', y='revenue')
chart.set_xticklabels(chart.get_xticklabels(), rotation=90, horizontalalignment='right', fontweight='light')
chart.axes.yaxis.label.set_text("revenue")

df2 = df.groupby('sector')[['profit']].mean().sort_values(['profit'],ascending=False)
df2.reset_index(inplace=True)
plt.figure(figsize=(10,10))
chart = sns.barplot(data = df2, x='sector', y='profit')
chart.set_xticklabels(chart.get_xticklabels(), rotation=90, horizontalalignment='right', fontweight='light')
chart.axes.yaxis.label.set_text("profit")

df2 = df.groupby('sector')[['profit']].mean().sort_values(['profit'],ascending=False)
df2.reset_index(inplace=True)
plt.figure(figsize=(15,15))
plt.pie(df2["profit"], labels = df2["sector"])
plt.show()

df2 = df.groupby('sector')[['rank']].mean().sort_values(['rank'],ascending=False)
df2.reset_index(inplace=True)
plt.figure(figsize=(15,15))
plt.pie(df2["rank"], labels = df2["sector"])
plt.show()

"""#Auditing
Clearing data and creating csv to move data to SQL Database
"""

# Mounting the drive
from google.colab import drive
drive.mount('/content/drive')

# Reading the scraped Data
df = pd.read_csv('Fortune_1000.csv')
df = df.replace(np.nan, 'NULL')

list_company = df['company'].to_list()
list_rank = df['rank'].to_list()
list_rank_change = df['rank_change'].to_list()
list_revenue = df['revenue'].to_list()
list_profit = df['profit'].to_list()
list_empCount = df['num. of employees'].to_list()
list_sector = df['sector'].to_list()
list_city  = df['city'].to_list()
list_state  = df['state'].to_list()
list_newcomer = df['newcomer'].to_list()
list_ceo_founder = df['ceo_founder'].to_list()
list_ceo_woman = df['ceo_woman'].to_list()
list_profitable = df['profitable'].to_list()
list_prev_rank = df['prev_rank'].to_list()
list_CEO = df['CEO'].to_list()
list_Website  = df['Website'].to_list()
list_Ticker  = df['Ticker'].to_list()
list_Market_Cap = df['Market Cap'].to_list()
database1 = {'company':list_company, 'rank':list_rank, 'rank_change':list_rank_change, 
             'revenue':list_revenue, 'profit':list_profit, 'empcount':list_empCount,'sector':list_sector, 
             'city':list_city, 'state': list_state} 
database2 = {'company':list_company,'newcomer':list_newcomer, 'ceo_founder':list_ceo_founder, 
             'ceo_woman':list_ceo_woman, 'profitable':list_profitable, 
             'prev_rank':list_prev_rank, 'CEO':list_CEO,'Website':list_Website, 
             'Ticker':list_Ticker, 'Market_Cap': list_Market_Cap}

df1=pd.DataFrame(database1)
df1.to_csv('Database1.csv', index=False)
df2=pd.DataFrame(database2)
df2.to_csv('Database2.csv', index=False)